# robots.txt for LichtFeld Studio
# https://lichtfeld.io
#
# This file tells search engine crawlers which pages to index.
# Reference: https://developers.google.com/search/docs/crawling-indexing/robots/intro

# Allow all crawlers to access all content
User-agent: *
Allow: /

# Sitemap location (adjust for custom domain vs GitHub Pages)
# The sitemap is generated by @astrojs/sitemap during build
Sitemap: https://lichtfeld.io/sitemap-index.xml

# Crawl-delay is optional and not supported by all crawlers
# Uncomment if you want to slow down aggressive bots
# Crawl-delay: 1
